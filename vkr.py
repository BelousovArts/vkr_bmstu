# -*- coding: utf-8 -*-
"""vkr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zOt1id2an_rSh8n4gVvK8RRyXTDIbtMT

## Структура

0. Импорт библиотек
1. Импорт STL файлов
2. Импорт CSV с результатами отклонений
3. Анализ и обработка данных (пропуски, выбросы, нормализация и стандартизация данных)
4. Архитектура и обучение нейронной сети
5. Анализ результатов

### 0. Импорт библиотек
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np  # работа с массивами
import pandas as pd  # работа с данными (таблицами)
import matplotlib.pyplot as plt  # визуализация
import seaborn as sns  # визуализация в статистике
from stl import mesh  # работа с STL моделями
import vtkplotlib as vpl  # визуализация STL

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
import tensorflow as tf

# %matplotlib inline

"""### 1. Импорт STL - моделей"""

mesh_initial = mesh.Mesh.from_file('model_stl\\VKR\\ini.stl')
mesh_deform = mesh.Mesh.from_file('model_stl\\VKR\\def.stl')
mesh_predef = mesh.Mesh.from_file('model_stl\\VKR\\ini.stl')
vpl.mesh_plot(mesh_initial)
vpl.mesh_plot(mesh_deform, color='red')
# vpl.show()

"""### 2. Импорт CSV-файлов с результатами отклонений"""

csv_data = pd.read_csv('model_stl\\VKR\\dev_full.asc',
                       sep=' ',
                       decimal=',',
                       names=['X','Y','Z','nx','ny','nz','dx','dy','dz','dev'])
csv_data.head()

"""### 3. Анализ и обработка данных (пропуски, масштабирование, выбросы, нормализация данных)"""

# описательная статистика
round(csv_data.describe(), 3)

# определение количества пропусков
csv_data.isna().sum()

# визуализируем пропуски
plt.figure(figsize=(10,10))
sns.heatmap(csv_data.isna(), cmap = 'viridis', cbar = False)
plt.show()

# удаление строк с пропущенными значениями
csv_data = csv_data.dropna()
csv_data.isna().sum()

# количественный анализ до обработки
plt.figure(figsize=(15,6))
sns.histplot(data=csv_data[['dx', 'dy', 'dz']], kde=True, binwidth=0.02)
plt.show()

# масштабирование, приведение к нормальному закому распределения
from sklearn.preprocessing import QuantileTransformer
quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=42)
csv_data_quantile = csv_data.copy()
csv_data_quantile[['dx', 'dy', 'dz']] = quantile_transformer.fit_transform(csv_data[['dx', 'dy', 'dz']])
csv_data_quantile.describe()

# количественный анализ после маштабирования
plt.figure(figsize=(15,6))
sns.histplot(data=csv_data_quantile[['dx', 'dy', 'dz']], kde=True, binwidth=0.2)
plt.show()

# ящики с усами до удаления выбросов
sns.set(style='whitegrid')
plt.figure(figsize=(15,6))
sns.boxplot(data=csv_data_quantile[['dx', 'dy', 'dz','dev']])

# график попарного распределения переменных до масштабирования
sns.set(style='whitegrid')
plt.figure(figsize=(10,5))
sns.pairplot(data=csv_data, plot_kws=dict(linewidth=0.2))
plt.show()

# удаление выбросов по методу 3-х сигм
low_dx = csv_data_quantile.dx.quantile(q=0.003)
upp_dx = csv_data_quantile.dx.quantile(q=0.997)
low_dy = csv_data_quantile.dy.quantile(q=0.003)
upp_dy = csv_data_quantile.dy.quantile(q=0.997)
low_dz = csv_data_quantile.dz.quantile(q=0.003)
upp_dz = csv_data_quantile.dz.quantile(q=0.997)

csv_data_quantile_n = csv_data_quantile.copy()
csv_data_quantile_n = csv_data_quantile[(csv_data_quantile.dx > low_dx) & (csv_data_quantile.dx < upp_dx) & (csv_data_quantile.dy > low_dy) & (csv_data_quantile.dy < upp_dy) & (csv_data_quantile.dz > low_dz) & (csv_data_quantile.dz < upp_dz)]

csv_data_quantile_n.describe()

# нормализация (-1, 1)
from sklearn.preprocessing import MinMaxScaler
minmax_scaler_X = MinMaxScaler(feature_range=(-1, 1))
minmax_scaler_y = MinMaxScaler(feature_range=(-1, 1))
csv_data_nrm = csv_data_quantile_n.copy()
csv_data_nrm[['X','Y','Z','nx','ny','nz','dev']] = minmax_scaler_X.fit_transform(csv_data_quantile_n[['X','Y','Z','nx','ny','nz','dev']])
csv_data_nrm[['dx','dy','dz']] = minmax_scaler_y.fit_transform(csv_data_quantile_n[['dx','dy','dz']])
csv_data_nrm.describe()

# количественный анализ после удаления выбросов и нормализации
plt.figure(figsize=(15,6))
sns.histplot(data=csv_data_nrm[['dx', 'dy', 'dz']], kde=True, binwidth=0.1)
plt.show()

# ящики с усами исходных данных
sns.set(style='whitegrid')
plt.figure(figsize=(15,6))
sns.boxplot(data=csv_data)

# ящики с усами после масштабирования, удаления выбросов и нормализации
sns.set(style='whitegrid')
plt.figure(figsize=(15,6))
sns.boxplot(data=csv_data_nrm)

# график попарного распределения переменных после масштабирования, удаления выбросов и нормализации
sns.set(style='whitegrid')
plt.figure(figsize=(10,5))
sns.pairplot(data=csv_data_nrm, plot_kws=dict(linewidth=0.2))
plt.show()

csv_data_nrm_corr = csv_data_nrm.corr()
plt.figure(figsize=(10,8))
sns.heatmap(data=csv_data_nrm_corr, vmin=-1, vmax=1, annot=True, cmap='coolwarm')
plt.show()

"""### 4. Архитектура и обучение нейронной сети"""

X = np.array(csv_data_nrm[['X', 'Y', 'Z']])  # входные данные
y = np.array(csv_data_nrm[['dx', 'dy', 'dz']])   # выходные данные


# разделение данных на тренировочную и тестовую выборку
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, shuffle = True, random_state=42)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# архитектура нейронной сети
model = Sequential()
model.add(Dense(3, input_dim = X_train.shape[1], activation='linear')) # входной слой
# model.add(BatchNormalization())
model.add(Dense(64, activation= 'relu'))
# model.add(BatchNormalization())
model.add(Dense(64, activation= 'relu'))
# model.add(Dropout(0.2))
model.add(Dense(64, activation= 'relu'))
model.add(Dense(3, activation= 'linear'))

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsoluteError()]) # компиляция
model.summary()

# Обучение нейронной сети
history = model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test), callbacks=[callback])
print(history.history.keys())

plt.plot(history.history['loss'], label = 'Ошибка на обучающей выборке')
plt.plot(history.history['val_loss'], label = 'Ошибка на тестовой выборке')
plt.xlabel('Эпохи')
plt.ylabel('Значение ошибки MSE')
plt.legend()
# plt.ylim(0,0.001)
plt.show()

"""### 5. Анализ результатов"""

# mesh_predef = mesh.Mesh.from_file('model_stl\\VKR\\ini.stl')

# нормализация данных для предсказания
mesh_predef_v0 = minmax_scaler_X.fit_transform(mesh_predef.v0)
mesh_predef_v1 = minmax_scaler_X.fit_transform(mesh_predef.v1)
mesh_predef_v2 = minmax_scaler_X.fit_transform(mesh_predef.v2)
mesh_predef_v0.min()

# предсказание
resV0 = pd.DataFrame()
resV1 = pd.DataFrame()
resV2 = pd.DataFrame()
resV0[['Dx', 'Dy', 'Dz']] = model.predict(mesh_predef_v0)
resV1[['Dx', 'Dy', 'Dz']] = model.predict(mesh_predef_v1)
resV2[['Dx', 'Dy', 'Dz']] = model.predict(mesh_predef_v2)

print(resV0.max())

# обратная нормализация
resV0 = minmax_scaler_y.inverse_transform(resV0)
resV1 = minmax_scaler_y.inverse_transform(resV1)
resV2 = minmax_scaler_y.inverse_transform(resV2)
resV0.min()

# обратное масштабирование
resV0 = quantile_transformer.inverse_transform(resV0)
resV1 = quantile_transformer.inverse_transform(resV1)
resV2 = quantile_transformer.inverse_transform(resV2)
resV0.min()

# предеформация 3d модели
mesh_predef.v0 = mesh_predef.v0 - resV0
mesh_predef.v1 = mesh_predef.v1 - resV1
mesh_predef.v2 = mesh_predef.v2 - resV2
mesh_predef.v0

# визуализация 3d моделей
vpl.mesh_plot(mesh_initial)
vpl.mesh_plot(mesh_deform, color='red')
vpl.mesh_plot(mesh_predef, color='green')
# vpl.show()

# сохраниение предеформированной модели
# mesh_predef.save('model_stl\\VKR\\predef.stl')